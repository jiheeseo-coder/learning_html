{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "기계학습10주차실습자료.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ygouBWJYCcY"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# 기계학습 10주차 실습 자료\n",
    "#### 조교 : 김도경\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1DvBToaY45G"
   },
   "source": [
    "#1. sklean이란?\n",
    "- Scikit-learn\n",
    "- 머신러닝에 사용되는 지도/비지도 학습 알고리즘을 제공하는 파이썬 라이브러리\n",
    "- Google Summer of Code 프로젝트로 2007년 시작 ~ January 2021. scikit-learn 0.24.1 버전까지 제공\n",
    "- https://scikit-learn.org/stable/\n",
    "- 이용 가이드\n",
    "  - https://scikit-learn.org/stable/user_guide.html\n",
    "- 예제\n",
    "  - https://scikit-learn.org/stable/auto_examples/index.html\n",
    "- gihub \n",
    "  - https://github.com/scikit-learn/scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qd7K2-QVYt09"
   },
   "source": [
    "# 2. OR 데이터 인식\n",
    "- OR 데이터에 퍼셉트론 적용"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eNeMLxnQYTnN",
    "outputId": "ddd85948-9f98-4330-975b-59891635ad58"
   },
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# 훈련 집합 구축\n",
    "\n",
    "X = [[0,0],[0,1],[1,0],[1,1]]\n",
    "y = [-1,1,1,1]\n",
    "11\n",
    "# fit 함수로 Perceptron 학습\n",
    "\n",
    "p = Perceptron()\n",
    "p.fit(X,y)\n",
    "\n",
    "print('학습된 퍼셉트론의 매개변수: ', p.coef_,p.intercept_)\n",
    "print('훈련집합에 대한 예측: ', p.predict(X))\n",
    "print('정확률 측정: ', p.score(X,y)*100, \"%\")"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습된 퍼셉트론의 매개변수:  [[2. 2.]] [-1.]\n",
      "훈련집합에 대한 예측:  [-1  1  1  1]\n",
      "정확률 측정:  100.0 %\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAt4kRQeajJA"
   },
   "source": [
    "# 3. 필기 숫자 데이터 인식\n",
    "## 3-1. sklearn이 제공하는 숫자 데이터에 퍼셉트론 적용\n",
    "- Perceptron 파라메터 \n",
    "  - max_iter : int, default=1000\n",
    "\n",
    "    The maximum number of passes over the training data (aka epochs).\n",
    "    It only impacts the behavior in the fit method, and not the\n",
    "    partial_fit method.\n",
    "  - verbose : int, default=0\n",
    "\n",
    "    The verbosity level\n",
    "\n",
    "  - eta0 : double, default=1\n",
    "\n",
    "    Constant by which the updates are multiplied.\n",
    "\n",
    "- 정확도 = 예측값결과와 실제값이 동일한 건수 / 전체 데이터수 "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kDOMJOTlZFd8",
    "outputId": "e46a3d00-315c-44fb-8970-ed4f2acc2f9f"
   },
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# 데이터셋을 읽고 훈련 집합과 테스트 집합으로 분할\n",
    "digit = datasets.load_digits()\n",
    "x_train, x_test, y_train, y_test = train_test_split(digit.data, digit.target, train_size = 0.6)\n",
    "\n",
    "# fit 함수로 Perceptron 학습\n",
    "p = Perceptron(max_iter = 100, eta0 = 0.001, verbose = 0) # 모델 객체 생성\n",
    "p.fit(x_train, y_train) # digit 데이터로 모델링 (모델 학습)\n",
    "\n",
    "res = p.predict(x_test) # 테스트 집합으로 예측 \n",
    "\n",
    "# 혼동 행렬 \n",
    "conf = np.zeros((10,10))\n",
    "\n",
    "for i in range(len(res)):\n",
    "  conf[res[i]][y_test[i]] += 1\n",
    "print(conf)\n",
    "\n",
    "# 정확률 계산\n",
    "no_correct = 0\n",
    "for i in range(10):\n",
    "  no_correct += conf[i][i]\n",
    "accuracy = no_correct/len(res)\n",
    "print('\\n테스트 집합에 대한 정확률은 ', accuracy*100,\"%입니다.\")"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[76.  0.  0.  0.  0.  0.  1.  0.  1.  0.]\n",
      " [ 0. 53.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  2. 69.  0.  0.  0.  0.  0.  0.  2.]\n",
      " [ 0.  2.  1. 74.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  3.  0.  0. 65.  0.  2.  1.  0.  0.]\n",
      " [ 0.  0.  0.  2.  0. 68.  0.  0.  0.  1.]\n",
      " [ 0.  2.  0.  0.  0.  1. 70.  0.  0.  0.]\n",
      " [ 0.  0.  0.  2.  0.  1.  0. 66.  0.  0.]\n",
      " [ 0. 13.  1.  2.  1.  0.  0.  1. 66.  3.]\n",
      " [ 0.  4.  0.  1.  0.  2.  0.  0.  0. 60.]]\n",
      "\n",
      "테스트 집합에 대한 정확률은  92.76773296244785 %입니다.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZUl103PwFAX"
   },
   "source": [
    "# 퍼셉트론의 한계\n",
    "\n",
    "- 선형 분리가 불가능한 데이터에서는 높은 오류율을 보인다\n",
    "- 은닉층을 추가한 다층 퍼셉트론으로 비선형 확장이 필요하다\n",
    "  - 퍼셉트론을 하나 더쓰면 XOR 문제를 풀 수 있다\n",
    "\n",
    "# 다층 퍼셉트론의 구조 \n",
    "\n",
    "- 입력층, 은닉층, 출력층으로 구성 \n",
    "  - 층을 연결하는 가중치 뭉치가 두개 있어 3층이 아닌 2층 신경망으로 간주됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMAqWsSyr9Yw"
   },
   "source": [
    "## 3-2. sklearn이 제공하는 숫자 데이터에 다층 퍼셉트론 적용\n",
    "\n",
    "- 하이퍼 매개변수란 ?\n",
    "  - 모델의 구조와 모델의 학습 과정을 제어하는 역할 \n",
    "---\n",
    "\n",
    "- MLPClassifier 함수의 하이퍼 매개변수\n",
    "  - hidden_layer_sizes = (100)\n",
    "    - 100개 노드를 가진 은닉층 한개를 둠 \n",
    "    - 100개와 80개 노드를 가진 은닉층 두개를 설정하려면 hidden_layer_sizes=(100,80)\n",
    "  - learning_rate_init = 0.001\n",
    "    - learning_rate를 0.00로 설정 \n",
    "  - batch_size = 32\n",
    "    - 미니 배치 크기를 32로 설정\n",
    "  - max_iter = 300\n",
    "    - 최대 반복을 300으로 설정\n",
    "  - solver = 'sgd'\n",
    "    - 최적화 알고리즘으로 스토캐스틱 경사 알고리즘을 사용\n",
    "  - verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74tvagGldBlt",
    "outputId": "55486d66-b59a-4234-99d0-86210cd92539"
   },
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# 데이터셋을 읽고 훈련 집합과 테스트 집합으로 분할\n",
    "digit = datasets.load_digits()\n",
    "x_train, x_test, y_train, y_test = train_test_split(digit.data, digit.target, train_size = 0.6)\n",
    "\n",
    "# MLP 분류기 모델을 학습\n",
    "mlp = MLPClassifier(hidden_layer_sizes = (100), learning_rate_init = 0.001, batch_size = 32, max_iter = 300, solver = 'sgd', verbose = True) # 모델 객체 생성\n",
    "mlp.fit(x_train, y_train) # digit 데이터로 모델링 (모델 학습)\n",
    "\n",
    "res = mlp.predict(x_test) # 테스트 집합으로 예측 \n",
    "\n",
    "# 혼동 행렬 \n",
    "conf = np.zeros((10,10))\n",
    "\n",
    "for i in range(len(res)):\n",
    "  conf[res[i]][y_test[i]] += 1\n",
    "print(conf)\n",
    "\n",
    "# 정확률 계산\n",
    "no_correct = 0\n",
    "for i in range(10):\n",
    "  no_correct += conf[i][i]\n",
    "accuracy = no_correct/len(res)\n",
    "print('\\n테스트 집합에 대한 정확률은 ', accuracy*100,\"%입니다.\")"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.32419751\n",
      "Iteration 2, loss = 0.31923012\n",
      "Iteration 3, loss = 0.20926782\n",
      "Iteration 4, loss = 0.16324749\n",
      "Iteration 5, loss = 0.12806690\n",
      "Iteration 6, loss = 0.10703706\n",
      "Iteration 7, loss = 0.09491709\n",
      "Iteration 8, loss = 0.08168506\n",
      "Iteration 9, loss = 0.07534916\n",
      "Iteration 10, loss = 0.06549842\n",
      "Iteration 11, loss = 0.06099887\n",
      "Iteration 12, loss = 0.05517300\n",
      "Iteration 13, loss = 0.05182664\n",
      "Iteration 14, loss = 0.04699411\n",
      "Iteration 15, loss = 0.04274841\n",
      "Iteration 16, loss = 0.04004611\n",
      "Iteration 17, loss = 0.03772022\n",
      "Iteration 18, loss = 0.03445131\n",
      "Iteration 19, loss = 0.03165312\n",
      "Iteration 20, loss = 0.03143489\n",
      "Iteration 21, loss = 0.02900459\n",
      "Iteration 22, loss = 0.02774230\n",
      "Iteration 23, loss = 0.02599011\n",
      "Iteration 24, loss = 0.02478012\n",
      "Iteration 25, loss = 0.02374583\n",
      "Iteration 26, loss = 0.02220715\n",
      "Iteration 27, loss = 0.02179454\n",
      "Iteration 28, loss = 0.02088197\n",
      "Iteration 29, loss = 0.02053743\n",
      "Iteration 30, loss = 0.01946761\n",
      "Iteration 31, loss = 0.01874325\n",
      "Iteration 32, loss = 0.01807008\n",
      "Iteration 33, loss = 0.01726474\n",
      "Iteration 34, loss = 0.01673296\n",
      "Iteration 35, loss = 0.01676147\n",
      "Iteration 36, loss = 0.01596440\n",
      "Iteration 37, loss = 0.01536304\n",
      "Iteration 38, loss = 0.01462301\n",
      "Iteration 39, loss = 0.01411031\n",
      "Iteration 40, loss = 0.01382182\n",
      "Iteration 41, loss = 0.01385846\n",
      "Iteration 42, loss = 0.01319596\n",
      "Iteration 43, loss = 0.01268897\n",
      "Iteration 44, loss = 0.01244933\n",
      "Iteration 45, loss = 0.01196036\n",
      "Iteration 46, loss = 0.01182962\n",
      "Iteration 47, loss = 0.01152591\n",
      "Iteration 48, loss = 0.01119327\n",
      "Iteration 49, loss = 0.01113531\n",
      "Iteration 50, loss = 0.01068844\n",
      "Iteration 51, loss = 0.01065591\n",
      "Iteration 52, loss = 0.01024162\n",
      "Iteration 53, loss = 0.01004378\n",
      "Iteration 54, loss = 0.00971530\n",
      "Iteration 55, loss = 0.00949885\n",
      "Iteration 56, loss = 0.00932788\n",
      "Iteration 57, loss = 0.00933084\n",
      "Iteration 58, loss = 0.00901950\n",
      "Iteration 59, loss = 0.00915559\n",
      "Iteration 60, loss = 0.00868547\n",
      "Iteration 61, loss = 0.00851891\n",
      "Iteration 62, loss = 0.00852184\n",
      "Iteration 63, loss = 0.00823739\n",
      "Iteration 64, loss = 0.00803085\n",
      "Iteration 65, loss = 0.00792979\n",
      "Iteration 66, loss = 0.00781973\n",
      "Iteration 67, loss = 0.00764998\n",
      "Iteration 68, loss = 0.00769193\n",
      "Iteration 69, loss = 0.00742890\n",
      "Iteration 70, loss = 0.00744755\n",
      "Iteration 71, loss = 0.00718402\n",
      "Iteration 72, loss = 0.00713558\n",
      "Iteration 73, loss = 0.00700009\n",
      "Iteration 74, loss = 0.00685580\n",
      "Iteration 75, loss = 0.00683407\n",
      "Iteration 76, loss = 0.00659713\n",
      "Iteration 77, loss = 0.00663668\n",
      "Iteration 78, loss = 0.00650100\n",
      "Iteration 79, loss = 0.00641772\n",
      "Iteration 80, loss = 0.00644027\n",
      "Iteration 81, loss = 0.00620275\n",
      "Iteration 82, loss = 0.00613772\n",
      "Iteration 83, loss = 0.00617579\n",
      "Iteration 84, loss = 0.00600725\n",
      "Iteration 85, loss = 0.00594574\n",
      "Iteration 86, loss = 0.00583572\n",
      "Iteration 87, loss = 0.00576707\n",
      "Iteration 88, loss = 0.00576974\n",
      "Iteration 89, loss = 0.00562425\n",
      "Iteration 90, loss = 0.00557674\n",
      "Iteration 91, loss = 0.00548324\n",
      "Iteration 92, loss = 0.00549877\n",
      "Iteration 93, loss = 0.00536458\n",
      "Iteration 94, loss = 0.00529711\n",
      "Iteration 95, loss = 0.00525120\n",
      "Iteration 96, loss = 0.00519335\n",
      "Iteration 97, loss = 0.00510532\n",
      "Iteration 98, loss = 0.00511777\n",
      "Iteration 99, loss = 0.00507733\n",
      "Iteration 100, loss = 0.00494169\n",
      "Iteration 101, loss = 0.00489056\n",
      "Iteration 102, loss = 0.00489600\n",
      "Iteration 103, loss = 0.00481288\n",
      "Iteration 104, loss = 0.00475976\n",
      "Iteration 105, loss = 0.00473532\n",
      "Iteration 106, loss = 0.00464796\n",
      "Iteration 107, loss = 0.00467063\n",
      "Iteration 108, loss = 0.00458558\n",
      "Iteration 109, loss = 0.00457445\n",
      "Iteration 110, loss = 0.00452919\n",
      "Iteration 111, loss = 0.00446557\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[[71.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0. 77.  0.  0.  0.  0.  1.  0.  2.  1.]\n",
      " [ 0.  0. 66.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. 71.  0.  0.  0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0. 74.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0. 72.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0. 78.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. 61.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0. 64.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0. 75.]]\n",
      "\n",
      "테스트 집합에 대한 정확률은  98.60917941585535 %입니다.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0R80CCrtOHf"
   },
   "source": [
    "# 4. MNIST 데이터셋으로 확장하기\n",
    "- MNIST 필기 숫자 데이터셋\n",
    "  - 훈련 집합 60000자 + 테스트 집합 10000자\n",
    "  - 샘플은 28*28 맵으로 표현 "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54FfOY_Yd2i1",
    "outputId": "300e7db4-0de7-45ee-8143-1f28017471e6"
   },
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# MNIST 데이터셋을 읽고 훈련 집합과 테스트 집합으로 분할\n",
    "mnist = fetch_openml('mnist_784')\n",
    "mnist.data = mnist.data/255.0 # [0,255] 범위를 [0,1] 범위로 변환\n",
    "x_train = mnist.data[:60000]\n",
    "x_test = mnist.data[60000:]\n",
    "y_train = np.int16(mnist.target[:60000])\n",
    "y_test = np.int16(mnist.target[60000:])\n",
    "\n",
    "# MLP 분류기 모델을 학습\n",
    "mlp = MLPClassifier(hidden_layer_sizes = (100), learning_rate_init = 0.001, batch_size = 512, max_iter = 300, solver = 'adam', verbose = True)\n",
    "mlp.fit(x_train, y_train) # digit 데이터로 모델링 (모델 학습)\n",
    "\n",
    "# 테스트 집합으로 예측\n",
    "res = mlp.predict(x_test)  \n",
    "\n",
    "# 혼동 행렬 \n",
    "conf = np.zeros((10,10))\n",
    "\n",
    "for i in range(len(res)):\n",
    "  conf[res[i]][y_test[i]] += 1\n",
    "print(conf)\n",
    "\n",
    "# 정확률 계산\n",
    "no_correct = 0\n",
    "for i in range(10):\n",
    "  no_correct += conf[i][i]\n",
    "accuracy = no_correct/len(res)\n",
    "print('\\n테스트 집합에 대한 정확률은 ', accuracy*100,\"%입니다.\")"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.60944217\n",
      "Iteration 2, loss = 0.25730312\n",
      "Iteration 3, loss = 0.20329726\n",
      "Iteration 4, loss = 0.16910290\n",
      "Iteration 5, loss = 0.14496842\n",
      "Iteration 6, loss = 0.12753874\n",
      "Iteration 7, loss = 0.11205003\n",
      "Iteration 8, loss = 0.10050731\n",
      "Iteration 9, loss = 0.09086324\n",
      "Iteration 10, loss = 0.08287398\n",
      "Iteration 11, loss = 0.07475398\n",
      "Iteration 12, loss = 0.06930070\n",
      "Iteration 13, loss = 0.06343666\n",
      "Iteration 14, loss = 0.05829960\n",
      "Iteration 15, loss = 0.05446004\n",
      "Iteration 16, loss = 0.05000703\n",
      "Iteration 17, loss = 0.04662390\n",
      "Iteration 18, loss = 0.04316967\n",
      "Iteration 19, loss = 0.04014949\n",
      "Iteration 20, loss = 0.03738237\n",
      "Iteration 21, loss = 0.03505576\n",
      "Iteration 22, loss = 0.03232063\n",
      "Iteration 23, loss = 0.03017817\n",
      "Iteration 24, loss = 0.02778149\n",
      "Iteration 25, loss = 0.02604714\n",
      "Iteration 26, loss = 0.02457820\n",
      "Iteration 27, loss = 0.02307881\n",
      "Iteration 28, loss = 0.02184803\n",
      "Iteration 29, loss = 0.01997990\n",
      "Iteration 30, loss = 0.01841497\n",
      "Iteration 31, loss = 0.01723460\n",
      "Iteration 32, loss = 0.01595886\n",
      "Iteration 33, loss = 0.01548231\n",
      "Iteration 34, loss = 0.01393171\n",
      "Iteration 35, loss = 0.01281492\n",
      "Iteration 36, loss = 0.01189099\n",
      "Iteration 37, loss = 0.01109641\n",
      "Iteration 38, loss = 0.01021910\n",
      "Iteration 39, loss = 0.00958413\n",
      "Iteration 40, loss = 0.00899825\n",
      "Iteration 41, loss = 0.00850047\n",
      "Iteration 42, loss = 0.00765879\n",
      "Iteration 43, loss = 0.00745294\n",
      "Iteration 44, loss = 0.00699844\n",
      "Iteration 45, loss = 0.00621142\n",
      "Iteration 46, loss = 0.00572629\n",
      "Iteration 47, loss = 0.00574679\n",
      "Iteration 48, loss = 0.00504188\n",
      "Iteration 49, loss = 0.00468370\n",
      "Iteration 50, loss = 0.00436593\n",
      "Iteration 51, loss = 0.00426076\n",
      "Iteration 52, loss = 0.00380490\n",
      "Iteration 53, loss = 0.00376198\n",
      "Iteration 54, loss = 0.00341180\n",
      "Iteration 55, loss = 0.00320260\n",
      "Iteration 56, loss = 0.00298688\n",
      "Iteration 57, loss = 0.00295800\n",
      "Iteration 58, loss = 0.00280536\n",
      "Iteration 59, loss = 0.00256947\n",
      "Iteration 60, loss = 0.00234634\n",
      "Iteration 61, loss = 0.00219788\n",
      "Iteration 62, loss = 0.00210489\n",
      "Iteration 63, loss = 0.00197030\n",
      "Iteration 64, loss = 0.00184494\n",
      "Iteration 65, loss = 0.00181861\n",
      "Iteration 66, loss = 0.00167644\n",
      "Iteration 67, loss = 0.00157824\n",
      "Iteration 68, loss = 0.00144576\n",
      "Iteration 69, loss = 0.00137228\n",
      "Iteration 70, loss = 0.00131123\n",
      "Iteration 71, loss = 0.00124923\n",
      "Iteration 72, loss = 0.00122843\n",
      "Iteration 73, loss = 0.00122094\n",
      "Iteration 74, loss = 0.00136106\n",
      "Iteration 75, loss = 0.00114618\n",
      "Iteration 76, loss = 0.01000035\n",
      "Iteration 77, loss = 0.00710564\n",
      "Iteration 78, loss = 0.00170705\n",
      "Iteration 79, loss = 0.00108061\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[[9.720e+02 0.000e+00 3.000e+00 0.000e+00 2.000e+00 2.000e+00 5.000e+00\n",
      "  1.000e+00 3.000e+00 2.000e+00]\n",
      " [0.000e+00 1.125e+03 2.000e+00 0.000e+00 1.000e+00 1.000e+00 2.000e+00\n",
      "  8.000e+00 0.000e+00 2.000e+00]\n",
      " [0.000e+00 2.000e+00 1.003e+03 2.000e+00 2.000e+00 0.000e+00 1.000e+00\n",
      "  8.000e+00 5.000e+00 0.000e+00]\n",
      " [1.000e+00 2.000e+00 2.000e+00 9.900e+02 1.000e+00 1.200e+01 1.000e+00\n",
      "  0.000e+00 5.000e+00 4.000e+00]\n",
      " [1.000e+00 0.000e+00 3.000e+00 0.000e+00 9.640e+02 1.000e+00 5.000e+00\n",
      "  1.000e+00 4.000e+00 1.100e+01]\n",
      " [1.000e+00 1.000e+00 0.000e+00 4.000e+00 0.000e+00 8.620e+02 4.000e+00\n",
      "  1.000e+00 2.000e+00 4.000e+00]\n",
      " [2.000e+00 2.000e+00 3.000e+00 0.000e+00 5.000e+00 5.000e+00 9.380e+02\n",
      "  0.000e+00 2.000e+00 0.000e+00]\n",
      " [1.000e+00 1.000e+00 6.000e+00 5.000e+00 0.000e+00 2.000e+00 0.000e+00\n",
      "  1.002e+03 4.000e+00 4.000e+00]\n",
      " [2.000e+00 2.000e+00 9.000e+00 4.000e+00 2.000e+00 6.000e+00 2.000e+00\n",
      "  1.000e+00 9.460e+02 4.000e+00]\n",
      " [0.000e+00 0.000e+00 1.000e+00 5.000e+00 5.000e+00 1.000e+00 0.000e+00\n",
      "  6.000e+00 3.000e+00 9.780e+02]]\n",
      "\n",
      "테스트 집합에 대한 정확률은  97.8 %입니다.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grduO35-yipP"
   },
   "source": [
    "- MLPClassifier는 원래 23개의 매개변수를 가짐 \n",
    "  - 따로 설정하지 않으면 기본값을 사용 \n",
    "  - max_iter = 300으로 설정했는데 86에서 학습을 멈춤 \n",
    "    - 기본 값중 tol = 0.0001과 n_iter_no_change = 10 때문에\n",
    "    - 10번 동안 손실 함수 감소량이 0.0001 이하면 멈추라는 뜻 \n",
    "\n",
    "- 하이퍼 매개변수 설정 가이드라인\n",
    "  - 논문이나 공식 문서를 참고 \n",
    "  - 라이브러리 함수가 제공하는 기본값 사용\n",
    "  - 중요한 하이퍼 매개변수를 골라 최적화\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eQNrwefHuhpN"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}