{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "기계학습10주차실습자료.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ygouBWJYCcY"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# 기계학습 10주차 실습 자료\n",
        "#### 조교 : 김도경\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1DvBToaY45G"
      },
      "source": [
        "#1. sklean이란?\n",
        "- Scikit-learn\n",
        "- 머신러닝에 사용되는 지도/비지도 학습 알고리즘을 제공하는 파이썬 라이브러리\n",
        "- Google Summer of Code 프로젝트로 2007년 시작 ~ January 2021. scikit-learn 0.24.1 버전까지 제공\n",
        "- https://scikit-learn.org/stable/\n",
        "- 이용 가이드\n",
        "  - https://scikit-learn.org/stable/user_guide.html\n",
        "- 예제\n",
        "  - https://scikit-learn.org/stable/auto_examples/index.html\n",
        "- gihub \n",
        "  - https://github.com/scikit-learn/scikit-learn\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd7K2-QVYt09"
      },
      "source": [
        "# 2. OR 데이터 인식\n",
        "- OR 데이터에 퍼셉트론 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNeMLxnQYTnN",
        "outputId": "ddd85948-9f98-4330-975b-59891635ad58"
      },
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "# 훈련 집합 구축\n",
        "\n",
        "X = [[0,0],[0,1],[1,0],[1,1]]\n",
        "y = [-1,1,1,1]\n",
        "11\n",
        "# fit 함수로 Perceptron 학습\n",
        "\n",
        "p = Perceptron()\n",
        "p.fit(X,y)\n",
        "\n",
        "print('학습된 퍼셉트론의 매개변수: ', p.coef_,p.intercept_)\n",
        "print('훈련집합에 대한 예측: ', p.predict(X))\n",
        "print('정확률 측정: ', p.score(X,y)*100, \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습된 퍼셉트론의 매개변수:  [[2. 2.]] [-1.]\n",
            "훈련집합에 대한 예측:  [-1  1  1  1]\n",
            "정확률 측정:  100.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAt4kRQeajJA"
      },
      "source": [
        "# 3. 필기 숫자 데이터 인식\n",
        "## 3-1. sklearn이 제공하는 숫자 데이터에 퍼셉트론 적용\n",
        "- Perceptron 파라메터 \n",
        "  - max_iter : int, default=1000\n",
        "\n",
        "    The maximum number of passes over the training data (aka epochs).\n",
        "    It only impacts the behavior in the fit method, and not the\n",
        "    partial_fit method.\n",
        "  - verbose : int, default=0\n",
        "\n",
        "    The verbosity level\n",
        "\n",
        "  - eta0 : double, default=1\n",
        "\n",
        "    Constant by which the updates are multiplied.\n",
        "\n",
        "- 정확도 = 예측값결과와 실제값이 동일한 건수 / 전체 데이터수 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDOMJOTlZFd8",
        "outputId": "e46a3d00-315c-44fb-8970-ed4f2acc2f9f"
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# 데이터셋을 읽고 훈련 집합과 테스트 집합으로 분할\n",
        "digit = datasets.load_digits()\n",
        "x_train, x_test, y_train, y_test = train_test_split(digit.data, digit.target, train_size = 0.6)\n",
        "\n",
        "# fit 함수로 Perceptron 학습\n",
        "p = Perceptron(max_iter = 100, eta0 = 0.001, verbose = 0) # 모델 객체 생성\n",
        "p.fit(x_train, y_train) # digit 데이터로 모델링 (모델 학습)\n",
        "\n",
        "res = p.predict(x_test) # 테스트 집합으로 예측 \n",
        "\n",
        "# 혼동 행렬 \n",
        "conf = np.zeros((10,10))\n",
        "\n",
        "for i in range(len(res)):\n",
        "  conf[res[i]][y_test[i]] += 1\n",
        "print(conf)\n",
        "\n",
        "# 정확률 계산\n",
        "no_correct = 0\n",
        "for i in range(10):\n",
        "  no_correct += conf[i][i]\n",
        "accuracy = no_correct/len(res)\n",
        "print('\\n테스트 집합에 대한 정확률은 ', accuracy*100,\"%입니다.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[74.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0. 56.  1.  0.  0.  1.  1.  0.  3.  4.]\n",
            " [ 0.  0. 68.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  4. 68.  0.  2.  0.  0.  4.  5.]\n",
            " [ 0.  1.  0.  0. 55.  0.  0.  1.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0. 64.  0.  0.  0.  1.]\n",
            " [ 0.  0.  0.  0.  0.  0. 81.  0.  0.  0.]\n",
            " [ 0.  0.  0.  1.  0.  0.  0. 81.  1.  2.]\n",
            " [ 0.  1.  1.  5.  1.  0.  1.  0. 65.  3.]\n",
            " [ 0.  0.  0.  0.  0.  1.  0.  0.  0. 62.]]\n",
            "\n",
            "테스트 집합에 대한 정확률은  93.7413073713491 %입니다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZUl103PwFAX"
      },
      "source": [
        "# 퍼셉트론의 한계\n",
        "\n",
        "- 선형 분리가 불가능한 데이터에서는 높은 오류율을 보인다\n",
        "- 은닉층을 추가한 다층 퍼셉트론으로 비선형 확장이 필요하다\n",
        "  - 퍼셉트론을 하나 더쓰면 XOR 문제를 풀 수 있다\n",
        "\n",
        "# 다층 퍼셉트론의 구조 \n",
        "\n",
        "- 입력층, 은닉층, 출력층으로 구성 \n",
        "  - 층을 연결하는 가중치 뭉치가 두개 있어 3층이 아닌 2층 신경망으로 간주됨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMAqWsSyr9Yw"
      },
      "source": [
        "## 3-2. sklearn이 제공하는 숫자 데이터에 다층 퍼셉트론 적용\n",
        "\n",
        "- 하이퍼 매개변수란 ?\n",
        "  - 모델의 구조와 모델의 학습 과정을 제어하는 역할 \n",
        "---\n",
        "\n",
        "- MLPClassifier 함수의 하이퍼 매개변수\n",
        "  - hidden_layer_sizes = (100)\n",
        "    - 100개 노드를 가진 은닉층 한개를 둠 \n",
        "    - 100개와 80개 노드를 가진 은닉층 두개를 설정하려면 hidden_layer_sizes=(100,80)\n",
        "  - learning_rate_init = 0.001\n",
        "    - learning_rate를 0.00로 설정 \n",
        "  - batch_size = 32\n",
        "    - 미니 배치 크기를 32로 설정\n",
        "  - max_iter = 300\n",
        "    - 최대 반복을 300으로 설정\n",
        "  - solver = 'sgd'\n",
        "    - 최적화 알고리즘으로 스토캐스틱 경사 알고리즘을 사용\n",
        "  - verbose = True"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74tvagGldBlt",
        "outputId": "55486d66-b59a-4234-99d0-86210cd92539"
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# 데이터셋을 읽고 훈련 집합과 테스트 집합으로 분할\n",
        "digit = datasets.load_digits()\n",
        "x_train, x_test, y_train, y_test = train_test_split(digit.data, digit.target, train_size = 0.6)\n",
        "\n",
        "# MLP 분류기 모델을 학습\n",
        "mlp = MLPClassifier(hidden_layer_sizes = (100), learning_rate_init = 0.001, batch_size = 32, max_iter = 300, solver = 'sgd', verbose = True) # 모델 객체 생성\n",
        "mlp.fit(x_train, y_train) # digit 데이터로 모델링 (모델 학습)\n",
        "\n",
        "res = mlp.predict(x_test) # 테스트 집합으로 예측 \n",
        "\n",
        "# 혼동 행렬 \n",
        "conf = np.zeros((10,10))\n",
        "\n",
        "for i in range(len(res)):\n",
        "  conf[res[i]][y_test[i]] += 1\n",
        "print(conf)\n",
        "\n",
        "# 정확률 계산\n",
        "no_correct = 0\n",
        "for i in range(10):\n",
        "  no_correct += conf[i][i]\n",
        "accuracy = no_correct/len(res)\n",
        "print('\\n테스트 집합에 대한 정확률은 ', accuracy*100,\"%입니다.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 2.22055938\n",
            "Iteration 2, loss = 0.32964564\n",
            "Iteration 3, loss = 0.22272273\n",
            "Iteration 4, loss = 0.17193508\n",
            "Iteration 5, loss = 0.14861243\n",
            "Iteration 6, loss = 0.12352630\n",
            "Iteration 7, loss = 0.10471085\n",
            "Iteration 8, loss = 0.09178159\n",
            "Iteration 9, loss = 0.08457326\n",
            "Iteration 10, loss = 0.07092105\n",
            "Iteration 11, loss = 0.06455161\n",
            "Iteration 12, loss = 0.06000481\n",
            "Iteration 13, loss = 0.05283673\n",
            "Iteration 14, loss = 0.05026938\n",
            "Iteration 15, loss = 0.04588563\n",
            "Iteration 16, loss = 0.04277930\n",
            "Iteration 17, loss = 0.03979446\n",
            "Iteration 18, loss = 0.03777048\n",
            "Iteration 19, loss = 0.03511143\n",
            "Iteration 20, loss = 0.03365890\n",
            "Iteration 21, loss = 0.03214888\n",
            "Iteration 22, loss = 0.03036321\n",
            "Iteration 23, loss = 0.02901279\n",
            "Iteration 24, loss = 0.02871513\n",
            "Iteration 25, loss = 0.02687610\n",
            "Iteration 26, loss = 0.02469376\n",
            "Iteration 27, loss = 0.02503157\n",
            "Iteration 28, loss = 0.02401191\n",
            "Iteration 29, loss = 0.02194595\n",
            "Iteration 30, loss = 0.02373433\n",
            "Iteration 31, loss = 0.02111219\n",
            "Iteration 32, loss = 0.02000800\n",
            "Iteration 33, loss = 0.01911639\n",
            "Iteration 34, loss = 0.01856099\n",
            "Iteration 35, loss = 0.01761223\n",
            "Iteration 36, loss = 0.01757475\n",
            "Iteration 37, loss = 0.01726223\n",
            "Iteration 38, loss = 0.01642707\n",
            "Iteration 39, loss = 0.01642221\n",
            "Iteration 40, loss = 0.01570282\n",
            "Iteration 41, loss = 0.01475622\n",
            "Iteration 42, loss = 0.01467481\n",
            "Iteration 43, loss = 0.01359828\n",
            "Iteration 44, loss = 0.01352711\n",
            "Iteration 45, loss = 0.01315714\n",
            "Iteration 46, loss = 0.01292653\n",
            "Iteration 47, loss = 0.01259005\n",
            "Iteration 48, loss = 0.01261818\n",
            "Iteration 49, loss = 0.01228598\n",
            "Iteration 50, loss = 0.01168619\n",
            "Iteration 51, loss = 0.01165161\n",
            "Iteration 52, loss = 0.01150440\n",
            "Iteration 53, loss = 0.01108323\n",
            "Iteration 54, loss = 0.01087652\n",
            "Iteration 55, loss = 0.01085378\n",
            "Iteration 56, loss = 0.01021123\n",
            "Iteration 57, loss = 0.01024826\n",
            "Iteration 58, loss = 0.00997882\n",
            "Iteration 59, loss = 0.00966412\n",
            "Iteration 60, loss = 0.00968610\n",
            "Iteration 61, loss = 0.00970242\n",
            "Iteration 62, loss = 0.00934061\n",
            "Iteration 63, loss = 0.00910076\n",
            "Iteration 64, loss = 0.00898783\n",
            "Iteration 65, loss = 0.00888586\n",
            "Iteration 66, loss = 0.00855340\n",
            "Iteration 67, loss = 0.00845129\n",
            "Iteration 68, loss = 0.00842055\n",
            "Iteration 69, loss = 0.00830434\n",
            "Iteration 70, loss = 0.00797344\n",
            "Iteration 71, loss = 0.00795584\n",
            "Iteration 72, loss = 0.00795589\n",
            "Iteration 73, loss = 0.00784066\n",
            "Iteration 74, loss = 0.00760629\n",
            "Iteration 75, loss = 0.00747759\n",
            "Iteration 76, loss = 0.00734434\n",
            "Iteration 77, loss = 0.00727141\n",
            "Iteration 78, loss = 0.00710985\n",
            "Iteration 79, loss = 0.00711655\n",
            "Iteration 80, loss = 0.00697626\n",
            "Iteration 81, loss = 0.00689179\n",
            "Iteration 82, loss = 0.00677538\n",
            "Iteration 83, loss = 0.00667042\n",
            "Iteration 84, loss = 0.00654245\n",
            "Iteration 85, loss = 0.00643320\n",
            "Iteration 86, loss = 0.00638944\n",
            "Iteration 87, loss = 0.00636311\n",
            "Iteration 88, loss = 0.00637395\n",
            "Iteration 89, loss = 0.00620628\n",
            "Iteration 90, loss = 0.00614305\n",
            "Iteration 91, loss = 0.00599117\n",
            "Iteration 92, loss = 0.00588818\n",
            "Iteration 93, loss = 0.00581273\n",
            "Iteration 94, loss = 0.00589362\n",
            "Iteration 95, loss = 0.00578365\n",
            "Iteration 96, loss = 0.00564557\n",
            "Iteration 97, loss = 0.00554734\n",
            "Iteration 98, loss = 0.00554431\n",
            "Iteration 99, loss = 0.00550554\n",
            "Iteration 100, loss = 0.00542019\n",
            "Iteration 101, loss = 0.00536489\n",
            "Iteration 102, loss = 0.00529322\n",
            "Iteration 103, loss = 0.00520756\n",
            "Iteration 104, loss = 0.00521413\n",
            "Iteration 105, loss = 0.00517957\n",
            "Iteration 106, loss = 0.00509718\n",
            "Iteration 107, loss = 0.00504154\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[[73.  0.  0.  0.  1.  0.  2.  0.  0.  0.]\n",
            " [ 0. 69.  3.  0.  0.  1.  1.  0.  1.  0.]\n",
            " [ 0.  0. 66.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. 76.  0.  0.  0.  0.  1.  1.]\n",
            " [ 0.  0.  0.  0. 71.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  1.  0. 65.  0.  0.  1.  1.]\n",
            " [ 1.  0.  0.  0.  0.  1. 76.  0.  0.  0.]\n",
            " [ 0.  0.  0.  1.  1.  1.  0. 69.  0.  0.]\n",
            " [ 0.  0.  0.  1.  0.  0.  0.  1. 62.  1.]\n",
            " [ 0.  1.  0.  2.  0.  1.  0.  1.  0. 65.]]\n",
            "\n",
            "테스트 집합에 대한 정확률은  96.24478442280946 %입니다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0R80CCrtOHf"
      },
      "source": [
        "# 4. MNIST 데이터셋으로 확장하기\n",
        "- MNIST 필기 숫자 데이터셋\n",
        "  - 훈련 집합 60000자 + 테스트 집합 10000자\n",
        "  - 샘플은 28*28 맵으로 표현 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54FfOY_Yd2i1",
        "outputId": "300e7db4-0de7-45ee-8143-1f28017471e6"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# MNIST 데이터셋을 읽고 훈련 집합과 테스트 집합으로 분할\n",
        "mnist = fetch_openml('mnist_784')\n",
        "mnist.data = mnist.data/255.0 # [0,255] 범위를 [0,1] 범위로 변환\n",
        "x_train = mnist.data[:60000]\n",
        "x_test = mnist.data[60000:]\n",
        "y_train = np.int16(mnist.target[:60000])\n",
        "y_test = np.int16(mnist.target[60000:])\n",
        "\n",
        "# MLP 분류기 모델을 학습\n",
        "mlp = MLPClassifier(hidden_layer_sizes = (100), learning_rate_init = 0.001, batch_size = 512, max_iter = 300, solver = 'adam', verbose = True)\n",
        "mlp.fit(x_train, y_train) # digit 데이터로 모델링 (모델 학습)\n",
        "\n",
        "# 테스트 집합으로 예측\n",
        "res = mlp.predict(x_test)  \n",
        "\n",
        "# 혼동 행렬 \n",
        "conf = np.zeros((10,10))\n",
        "\n",
        "for i in range(len(res)):\n",
        "  conf[res[i]][y_test[i]] += 1\n",
        "print(conf)\n",
        "\n",
        "# 정확률 계산\n",
        "no_correct = 0\n",
        "for i in range(10):\n",
        "  no_correct += conf[i][i]\n",
        "accuracy = no_correct/len(res)\n",
        "print('\\n테스트 집합에 대한 정확률은 ', accuracy*100,\"%입니다.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.62838595\n",
            "Iteration 2, loss = 0.26737789\n",
            "Iteration 3, loss = 0.21198174\n",
            "Iteration 4, loss = 0.17811898\n",
            "Iteration 5, loss = 0.15395003\n",
            "Iteration 6, loss = 0.13514445\n",
            "Iteration 7, loss = 0.11933278\n",
            "Iteration 8, loss = 0.10697935\n",
            "Iteration 9, loss = 0.09607583\n",
            "Iteration 10, loss = 0.08754710\n",
            "Iteration 11, loss = 0.07871194\n",
            "Iteration 12, loss = 0.07213644\n",
            "Iteration 13, loss = 0.06637970\n",
            "Iteration 14, loss = 0.06066776\n",
            "Iteration 15, loss = 0.05610347\n",
            "Iteration 16, loss = 0.05211771\n",
            "Iteration 17, loss = 0.04812897\n",
            "Iteration 18, loss = 0.04471605\n",
            "Iteration 19, loss = 0.04203831\n",
            "Iteration 20, loss = 0.03827570\n",
            "Iteration 21, loss = 0.03542483\n",
            "Iteration 22, loss = 0.03304306\n",
            "Iteration 23, loss = 0.03065604\n",
            "Iteration 24, loss = 0.02825335\n",
            "Iteration 25, loss = 0.02673786\n",
            "Iteration 26, loss = 0.02522631\n",
            "Iteration 27, loss = 0.02307002\n",
            "Iteration 28, loss = 0.02228705\n",
            "Iteration 29, loss = 0.02048527\n",
            "Iteration 30, loss = 0.01871350\n",
            "Iteration 31, loss = 0.01732744\n",
            "Iteration 32, loss = 0.01584542\n",
            "Iteration 33, loss = 0.01526054\n",
            "Iteration 34, loss = 0.01400545\n",
            "Iteration 35, loss = 0.01303156\n",
            "Iteration 36, loss = 0.01238787\n",
            "Iteration 37, loss = 0.01148301\n",
            "Iteration 38, loss = 0.01079053\n",
            "Iteration 39, loss = 0.00996445\n",
            "Iteration 40, loss = 0.00908187\n",
            "Iteration 41, loss = 0.00860674\n",
            "Iteration 42, loss = 0.00802071\n",
            "Iteration 43, loss = 0.00748601\n",
            "Iteration 44, loss = 0.00698263\n",
            "Iteration 45, loss = 0.00643232\n",
            "Iteration 46, loss = 0.00602335\n",
            "Iteration 47, loss = 0.00566235\n",
            "Iteration 48, loss = 0.00522597\n",
            "Iteration 49, loss = 0.00482976\n",
            "Iteration 50, loss = 0.00450906\n",
            "Iteration 51, loss = 0.00431321\n",
            "Iteration 52, loss = 0.00396111\n",
            "Iteration 53, loss = 0.00349705\n",
            "Iteration 54, loss = 0.00342660\n",
            "Iteration 55, loss = 0.00309872\n",
            "Iteration 56, loss = 0.00301516\n",
            "Iteration 57, loss = 0.00279252\n",
            "Iteration 58, loss = 0.00269815\n",
            "Iteration 59, loss = 0.00243901\n",
            "Iteration 60, loss = 0.00235606\n",
            "Iteration 61, loss = 0.00215091\n",
            "Iteration 62, loss = 0.00198230\n",
            "Iteration 63, loss = 0.00191555\n",
            "Iteration 64, loss = 0.00181806\n",
            "Iteration 65, loss = 0.00170483\n",
            "Iteration 66, loss = 0.00163817\n",
            "Iteration 67, loss = 0.00148702\n",
            "Iteration 68, loss = 0.00139920\n",
            "Iteration 69, loss = 0.00130320\n",
            "Iteration 70, loss = 0.00130605\n",
            "Iteration 71, loss = 0.00122276\n",
            "Iteration 72, loss = 0.00112574\n",
            "Iteration 73, loss = 0.00109476\n",
            "Iteration 74, loss = 0.00109736\n",
            "Iteration 75, loss = 0.00098881\n",
            "Iteration 76, loss = 0.00100880\n",
            "Iteration 77, loss = 0.00111674\n",
            "Iteration 78, loss = 0.00086949\n",
            "Iteration 79, loss = 0.00083189\n",
            "Iteration 80, loss = 0.00077426\n",
            "Iteration 81, loss = 0.00075095\n",
            "Iteration 82, loss = 0.00071165\n",
            "Iteration 83, loss = 0.00071036\n",
            "Iteration 84, loss = 0.00066326\n",
            "Iteration 85, loss = 0.00064090\n",
            "Iteration 86, loss = 0.00063586\n",
            "Iteration 87, loss = 0.00059222\n",
            "Iteration 88, loss = 0.00056216\n",
            "Iteration 89, loss = 0.00061169\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[[9.700e+02 0.000e+00 3.000e+00 0.000e+00 2.000e+00 3.000e+00 6.000e+00\n",
            "  1.000e+00 5.000e+00 2.000e+00]\n",
            " [1.000e+00 1.124e+03 2.000e+00 1.000e+00 1.000e+00 1.000e+00 2.000e+00\n",
            "  6.000e+00 0.000e+00 3.000e+00]\n",
            " [1.000e+00 2.000e+00 1.000e+03 5.000e+00 3.000e+00 0.000e+00 2.000e+00\n",
            "  8.000e+00 3.000e+00 0.000e+00]\n",
            " [0.000e+00 2.000e+00 5.000e+00 9.850e+02 1.000e+00 1.200e+01 1.000e+00\n",
            "  2.000e+00 2.000e+00 5.000e+00]\n",
            " [0.000e+00 0.000e+00 2.000e+00 0.000e+00 9.640e+02 2.000e+00 2.000e+00\n",
            "  0.000e+00 3.000e+00 1.000e+01]\n",
            " [1.000e+00 1.000e+00 0.000e+00 4.000e+00 0.000e+00 8.660e+02 5.000e+00\n",
            "  0.000e+00 3.000e+00 1.000e+00]\n",
            " [3.000e+00 2.000e+00 4.000e+00 0.000e+00 3.000e+00 2.000e+00 9.370e+02\n",
            "  0.000e+00 2.000e+00 0.000e+00]\n",
            " [1.000e+00 1.000e+00 5.000e+00 6.000e+00 2.000e+00 2.000e+00 1.000e+00\n",
            "  1.003e+03 4.000e+00 6.000e+00]\n",
            " [3.000e+00 3.000e+00 1.000e+01 3.000e+00 1.000e+00 4.000e+00 2.000e+00\n",
            "  3.000e+00 9.500e+02 4.000e+00]\n",
            " [0.000e+00 0.000e+00 1.000e+00 6.000e+00 5.000e+00 0.000e+00 0.000e+00\n",
            "  5.000e+00 2.000e+00 9.780e+02]]\n",
            "\n",
            "테스트 집합에 대한 정확률은  97.77 %입니다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grduO35-yipP"
      },
      "source": [
        "- MLPClassifier는 원래 23개의 매개변수를 가짐 \n",
        "  - 따로 설정하지 않으면 기본값을 사용 \n",
        "  - max_iter = 300으로 설정했는데 86에서 학습을 멈춤 \n",
        "    - 기본 값중 tol = 0.0001과 n_iter_no_change = 10 때문에\n",
        "    - 10번 동안 손실 함수 감소량이 0.0001 이하면 멈추라는 뜻 \n",
        "\n",
        "- 하이퍼 매개변수 설정 가이드라인\n",
        "  - 논문이나 공식 문서를 참고 \n",
        "  - 라이브러리 함수가 제공하는 기본값 사용\n",
        "  - 중요한 하이퍼 매개변수를 골라 최적화\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQNrwefHuhpN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}